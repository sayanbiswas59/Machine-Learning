{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(\"spambase.names\", 'r')\n",
    "# count = 0\n",
    "# column_names = []\n",
    "# for line in f:\n",
    "#     count = count+1\n",
    "#     if count >=34:\n",
    "#         temp = line.split(\":\")\n",
    "#         column_names.append(temp[0])\n",
    "# print(len(column_names))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['word_freq_make', 'word_freq_address', 'word_freq_all', 'word_freq_3d', 'word_freq_our', 'word_freq_over', 'word_freq_remove',\n",
    " 'word_freq_internet', 'word_freq_order', 'word_freq_mail', 'word_freq_receive', 'word_freq_will', 'word_freq_people', \n",
    " 'word_freq_report', 'word_freq_addresses', 'word_freq_free', 'word_freq_business', 'word_freq_email', 'word_freq_you', \n",
    " 'word_freq_credit', 'word_freq_your', 'word_freq_font', 'word_freq_000', 'word_freq_money', 'word_freq_hp', 'word_freq_hpl', \n",
    " 'word_freq_george', 'word_freq_650', 'word_freq_lab', 'word_freq_labs', 'word_freq_telnet', 'word_freq_857', 'word_freq_data', \n",
    " 'word_freq_415', 'word_freq_85', 'word_freq_technology', 'word_freq_1999', 'word_freq_parts', 'word_freq_pm', 'word_freq_direct', \n",
    " 'word_freq_cs', 'word_freq_meeting', 'word_freq_original', 'word_freq_project', 'word_freq_re', 'word_freq_edu', 'word_freq_table',\n",
    " 'word_freq_conference', 'char_freq_;', 'char_freq_(', 'char_freq_[', 'char_freq_!', 'char_freq_$', 'char_freq_#', 'capital_run_length_average', \n",
    " 'capital_run_length_longest', 'capital_run_length_total', 'class']\n",
    "org_data = pd.read_csv(\"spambase.data\",names = column_names)\n",
    "#org_data.iloc[:,0:-1]\n",
    "#org_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = org_data.iloc[:,0:-1]\n",
    "y = org_data[['class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, stratify=y, random_state = 57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "ss_scaler = preprocessing.StandardScaler()\n",
    "X_train = pd.DataFrame(ss_scaler.fit_transform(X_train),columns=X_train.columns)\n",
    "X_test = pd.DataFrame(ss_scaler.transform(X_test),columns = X_test.columns)\n",
    "y_train = np.ravel(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check of the proportion of the class after splitting\n",
    "\n",
    "# y_train.sum(axis = 0, skipna = True)[0]/y_train.shape[0]\n",
    "# y_test.sum(axis = 0, skipna = True)[0]/y_test.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[667  30]\n",
      " [ 42 412]]\n"
     ]
    }
   ],
   "source": [
    "# 1. Confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 412 , False Positives:  30 , True Negatives:  667 , False Negatives:  42\n"
     ]
    }
   ],
   "source": [
    "# 2. True Positives, False Positives, True Negatives, False Negatives\n",
    "true_negatives = conf_mat[0][0]\n",
    "false_negatives = conf_mat[1][0]\n",
    "true_positives = conf_mat[1][1]\n",
    "false_positives = conf_mat[0][1]\n",
    "print(\"True Positives:\", true_positives, \", False Positives: \", false_positives, \", True Negatives: \", true_negatives,\n",
    "     \", False Negatives: \", false_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9374456993918332 , Error:  0.0625543006081668\n"
     ]
    }
   ],
   "source": [
    "# 3. Accuracy, Error\n",
    "accuracy = (true_positives + true_negatives)/y_test.shape[0]\n",
    "error = 1-accuracy\n",
    "print(\"Accuracy: \", accuracy, \", Error: \", error)\n",
    "#print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(log_reg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision on testing set: 0.9321266968325792\n",
      "Recall on testing set: 0.9074889867841409\n",
      "F1-score on testing set: 0.9196428571428571\n"
     ]
    }
   ],
   "source": [
    "#print(\"Accuracy on testing set:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision on testing set:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall on testing set:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"F1-score on testing set:\",metrics.f1_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [-2.05031456]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>word_freq_make</td>\n",
       "      <td>-0.128952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_address</td>\n",
       "      <td>-0.229919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_all</td>\n",
       "      <td>0.021797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_3d</td>\n",
       "      <td>0.864687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_our</td>\n",
       "      <td>0.375941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_over</td>\n",
       "      <td>0.162546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_remove</td>\n",
       "      <td>1.072679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_internet</td>\n",
       "      <td>0.227923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_order</td>\n",
       "      <td>0.184366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_mail</td>\n",
       "      <td>0.026171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_receive</td>\n",
       "      <td>0.052486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_will</td>\n",
       "      <td>-0.147935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_people</td>\n",
       "      <td>-0.066259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_report</td>\n",
       "      <td>0.027453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_addresses</td>\n",
       "      <td>0.327778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_free</td>\n",
       "      <td>0.851626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_business</td>\n",
       "      <td>0.313429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_email</td>\n",
       "      <td>0.107110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_you</td>\n",
       "      <td>0.147704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_credit</td>\n",
       "      <td>0.297572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_your</td>\n",
       "      <td>0.249094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_font</td>\n",
       "      <td>0.204894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_000</td>\n",
       "      <td>1.528238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_money</td>\n",
       "      <td>0.333764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_hp</td>\n",
       "      <td>-2.060978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_hpl</td>\n",
       "      <td>-0.840894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_george</td>\n",
       "      <td>-3.708191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_650</td>\n",
       "      <td>0.222779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_lab</td>\n",
       "      <td>-0.768961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_labs</td>\n",
       "      <td>-0.182240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_telnet</td>\n",
       "      <td>-0.225475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_857</td>\n",
       "      <td>0.337067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_data</td>\n",
       "      <td>-0.317132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_415</td>\n",
       "      <td>-1.268668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_85</td>\n",
       "      <td>-0.700761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_technology</td>\n",
       "      <td>0.249106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_1999</td>\n",
       "      <td>-0.025615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_parts</td>\n",
       "      <td>-0.134884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_pm</td>\n",
       "      <td>-0.380025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_direct</td>\n",
       "      <td>-0.341114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_cs</td>\n",
       "      <td>-1.563493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_meeting</td>\n",
       "      <td>-1.479615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_original</td>\n",
       "      <td>-0.178957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_project</td>\n",
       "      <td>-0.753836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_re</td>\n",
       "      <td>-0.588157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_edu</td>\n",
       "      <td>-0.822307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_table</td>\n",
       "      <td>-0.176247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_conference</td>\n",
       "      <td>-0.998389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>char_freq_;</td>\n",
       "      <td>-0.288732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>char_freq_(</td>\n",
       "      <td>0.030094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>char_freq_[</td>\n",
       "      <td>-0.277017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>char_freq_!</td>\n",
       "      <td>0.656580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>char_freq_$</td>\n",
       "      <td>1.091596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>char_freq_#</td>\n",
       "      <td>0.612885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>capital_run_length_average</td>\n",
       "      <td>1.377071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>capital_run_length_longest</td>\n",
       "      <td>1.160721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>capital_run_length_total</td>\n",
       "      <td>0.344884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Coefficients\n",
       "word_freq_make                 -0.128952\n",
       "word_freq_address              -0.229919\n",
       "word_freq_all                   0.021797\n",
       "word_freq_3d                    0.864687\n",
       "word_freq_our                   0.375941\n",
       "word_freq_over                  0.162546\n",
       "word_freq_remove                1.072679\n",
       "word_freq_internet              0.227923\n",
       "word_freq_order                 0.184366\n",
       "word_freq_mail                  0.026171\n",
       "word_freq_receive               0.052486\n",
       "word_freq_will                 -0.147935\n",
       "word_freq_people               -0.066259\n",
       "word_freq_report                0.027453\n",
       "word_freq_addresses             0.327778\n",
       "word_freq_free                  0.851626\n",
       "word_freq_business              0.313429\n",
       "word_freq_email                 0.107110\n",
       "word_freq_you                   0.147704\n",
       "word_freq_credit                0.297572\n",
       "word_freq_your                  0.249094\n",
       "word_freq_font                  0.204894\n",
       "word_freq_000                   1.528238\n",
       "word_freq_money                 0.333764\n",
       "word_freq_hp                   -2.060978\n",
       "word_freq_hpl                  -0.840894\n",
       "word_freq_george               -3.708191\n",
       "word_freq_650                   0.222779\n",
       "word_freq_lab                  -0.768961\n",
       "word_freq_labs                 -0.182240\n",
       "word_freq_telnet               -0.225475\n",
       "word_freq_857                   0.337067\n",
       "word_freq_data                 -0.317132\n",
       "word_freq_415                  -1.268668\n",
       "word_freq_85                   -0.700761\n",
       "word_freq_technology            0.249106\n",
       "word_freq_1999                 -0.025615\n",
       "word_freq_parts                -0.134884\n",
       "word_freq_pm                   -0.380025\n",
       "word_freq_direct               -0.341114\n",
       "word_freq_cs                   -1.563493\n",
       "word_freq_meeting              -1.479615\n",
       "word_freq_original             -0.178957\n",
       "word_freq_project              -0.753836\n",
       "word_freq_re                   -0.588157\n",
       "word_freq_edu                  -0.822307\n",
       "word_freq_table                -0.176247\n",
       "word_freq_conference           -0.998389\n",
       "char_freq_;                    -0.288732\n",
       "char_freq_(                     0.030094\n",
       "char_freq_[                    -0.277017\n",
       "char_freq_!                     0.656580\n",
       "char_freq_$                     1.091596\n",
       "char_freq_#                     0.612885\n",
       "capital_run_length_average      1.377071\n",
       "capital_run_length_longest      1.160721\n",
       "capital_run_length_total        0.344884"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff = pd.DataFrame(log_reg.coef_.T,X_train.columns,columns=['Coefficients'])\n",
    "print(\"Intercept:\", log_reg.intercept_)\n",
    "coeff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features that contribute mostly to the prediction are given below:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>word_freq_george</td>\n",
       "      <td>-3.708191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_hp</td>\n",
       "      <td>-2.060978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_cs</td>\n",
       "      <td>-1.563493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_000</td>\n",
       "      <td>1.528238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_meeting</td>\n",
       "      <td>-1.479615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>capital_run_length_average</td>\n",
       "      <td>1.377071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_415</td>\n",
       "      <td>-1.268668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>capital_run_length_longest</td>\n",
       "      <td>1.160721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>char_freq_$</td>\n",
       "      <td>1.091596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_remove</td>\n",
       "      <td>1.072679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Coefficients\n",
       "word_freq_george               -3.708191\n",
       "word_freq_hp                   -2.060978\n",
       "word_freq_cs                   -1.563493\n",
       "word_freq_000                   1.528238\n",
       "word_freq_meeting              -1.479615\n",
       "capital_run_length_average      1.377071\n",
       "word_freq_415                  -1.268668\n",
       "capital_run_length_longest      1.160721\n",
       "char_freq_$                     1.091596\n",
       "word_freq_remove                1.072679"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff[\"new_coeff\"] = abs(coeff[\"Coefficients\"])\n",
    "coeff = coeff.sort_values(by = 'new_coeff',ascending=False)\n",
    "coeff = coeff.drop('new_coeff',axis =1)\n",
    "print(\"The features that contribute mostly to the prediction are given below:\")\n",
    "coeff.head(n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features positively correlated with spam class:  ['word_freq_all', 'word_freq_3d', 'word_freq_our', 'word_freq_over', 'word_freq_remove', 'word_freq_internet', 'word_freq_order', 'word_freq_mail', 'word_freq_receive', 'word_freq_report', 'word_freq_addresses', 'word_freq_free', 'word_freq_business', 'word_freq_email', 'word_freq_you', 'word_freq_credit', 'word_freq_your', 'word_freq_font', 'word_freq_000', 'word_freq_money', 'word_freq_650', 'word_freq_857', 'word_freq_technology', 'char_freq_(', 'char_freq_!', 'char_freq_$', 'char_freq_#', 'capital_run_length_average', 'capital_run_length_longest', 'capital_run_length_total']\n",
      "\n",
      "\n",
      "Features negatively correlated with spam class:  ['word_freq_make', 'word_freq_address', 'word_freq_will', 'word_freq_people', 'word_freq_hp', 'word_freq_hpl', 'word_freq_george', 'word_freq_lab', 'word_freq_labs', 'word_freq_telnet', 'word_freq_data', 'word_freq_415', 'word_freq_85', 'word_freq_1999', 'word_freq_parts', 'word_freq_pm', 'word_freq_direct', 'word_freq_cs', 'word_freq_meeting', 'word_freq_original', 'word_freq_project', 'word_freq_re', 'word_freq_edu', 'word_freq_table', 'word_freq_conference', 'char_freq_;', 'char_freq_[']\n"
     ]
    }
   ],
   "source": [
    "#coeff = pd.DataFrame(log_reg.coef_.T,X_train.columns,columns=['Coefficients'])\n",
    "coeff = pd.DataFrame(log_reg.coef_,columns = X_train.columns)\n",
    "pos_corr = []\n",
    "neg_corr = []\n",
    "for i in coeff:\n",
    "    if coeff[i][0] > 0:\n",
    "        pos_corr.append(i)\n",
    "    else:\n",
    "        neg_corr.append(i)\n",
    "print(\"Features positively correlated with spam class: \", pos_corr)\n",
    "print(\"\\n\")\n",
    "print(\"Features negatively correlated with spam class: \", neg_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_reg.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy when decision threshold is  0.25 :  0.9148566463944396\n",
      "Precision when decision threshold is  0.25 :  0.8503937007874016\n",
      "Recall when decision threshold is  0.25 :  0.9515418502202643\n",
      "\n",
      "\n",
      "Accuracy when decision threshold is  0.5 :  0.9374456993918332\n",
      "Precision when decision threshold is  0.5 :  0.9321266968325792\n",
      "Recall when decision threshold is  0.5 :  0.9074889867841409\n",
      "\n",
      "\n",
      "Accuracy when decision threshold is  0.75 :  0.895742832319722\n",
      "Precision when decision threshold is  0.75 :  0.9441489361702128\n",
      "Recall when decision threshold is  0.75 :  0.7819383259911894\n",
      "\n",
      "\n",
      "Accuracy when decision threshold is  0.9 :  0.8479582971329279\n",
      "Precision when decision threshold is  0.9 :  0.9603960396039604\n",
      "Recall when decision threshold is  0.9 :  0.6409691629955947\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_prob = log_reg.predict_proba(X_test)[::,1]\n",
    "\n",
    "\n",
    "\n",
    "decision_threshold = [0.25,0.5,0.75,0.9]\n",
    "\n",
    "for i in decision_threshold:\n",
    "    y_pred_dt = []\n",
    "    for j in y_prob:\n",
    "        if j>=i:\n",
    "            y_pred_dt.append(1)\n",
    "        else:\n",
    "            y_pred_dt.append(0)\n",
    "    print(\"Accuracy when decision threshold is \",i,\": \",metrics.accuracy_score(y_test, y_pred_dt))\n",
    "    print(\"Precision when decision threshold is \",i,\": \",metrics.precision_score(y_test, y_pred_dt))\n",
    "    print(\"Recall when decision threshold is \",i,\": \",metrics.recall_score(y_test, y_pred_dt))\n",
    "    #print(confusion_matrix(y_test, y_pred_dt))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For decision threshold = 0.5, the combination of the values of the accuracy, precision and recall are best, hence the decision threshold = 0.5 is the best choice. Accuracy of the model increases as the threshold increases till threshold=0.5, post which the accuracy decreases. Precision increases as the decision threshold increases as the count of false positives decreases. Recall decreases as the threshold increases as the count of false negatives increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = org_data.iloc[:,0:-1]\n",
    "y = org_data[['class']]\n",
    "\n",
    "# splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, stratify=y, random_state = 57)\n",
    "\n",
    "# preprocessing\n",
    "ss_scaler = preprocessing.StandardScaler()\n",
    "X_train = pd.DataFrame(ss_scaler.fit_transform(X_train),columns=X_train.columns)\n",
    "X_test = pd.DataFrame(ss_scaler.transform(X_test),columns = X_test.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Cost function is implemented to check if the cost is decreasing with iterations\n",
    "def cross_entropy_obj(X,y,theta):\n",
    "    N = y.shape[0]\n",
    "    z = np.dot(X, theta)\n",
    "    h = sigmoid(z)\n",
    "#     print(\"h\",h)\n",
    "#     print(\"first half\", -(np.transpose(y).dot(np.log(h))))\n",
    "#     print(\"second half\", -(np.transpose(1-y).dot(np.log(1-h))))\n",
    "    cost = (1/N)*(-(np.transpose(y).dot(np.log(h)))-(np.transpose(1-y).dot(np.log(1-h))))\n",
    "    return cost\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Method to compute the theta values using gradient descent\n",
    "def gradient_descent_log_reg(X, y, alpha, num_iters):\n",
    "    iters = num_iters\n",
    "    if 'x0' not in X.columns:\n",
    "        x0 = np.ones((X.shape[0], 1), dtype=int)\n",
    "        X.insert(0, \"x0\", x0, True)\n",
    "    # d is the number of features    \n",
    "    d = X_train.shape[1]\n",
    "    # Initializing theta with zeros\n",
    "    theta = np.zeros((d,1))    \n",
    "    X=X.values\n",
    "    y=y.values\n",
    "    diff_cost = 0\n",
    "    N = y.shape[0]\n",
    "    theta_old = theta\n",
    "    \n",
    "    #while diff_cost > 0.000001 or num_iters > 0\n",
    "    \n",
    "    while num_iters > 0:\n",
    "        z = np.dot(X, theta_old)\n",
    "        h = sigmoid(z)\n",
    "        gradient = np.transpose(X).dot(h-y)\n",
    "        theta_new = theta_old - alpha * (1/N) * gradient\n",
    "        #print(\"theta_new\", theta_new)\n",
    "        # checking for convergence\n",
    "        delta_theta = theta_new - theta_old\n",
    "        delta = np.sqrt(np.transpose(delta_theta).dot(delta_theta))\n",
    "        #print(delta)\n",
    "        if delta < 0.00001 and iters != num_iters :\n",
    "            print(\"Gradient descent converged at iteration\", iters-num_iters)\n",
    "            break\n",
    "        num_iters = num_iters - 1\n",
    "        old_cost = cross_entropy_obj(X,y,theta_old)\n",
    "        theta_old = theta_new\n",
    "        new_cost = cross_entropy_obj(X,y,theta_new)\n",
    "        diff_cost = old_cost - new_cost\n",
    "        #print(\"old_cost\", old_cost)\n",
    "        #print(\"new cost\", new_cost)\n",
    "        #print(\"diffcost:\", diff_cost)\n",
    "    return theta_new\n",
    "\n",
    "\n",
    "def predict_prob(X,theta):\n",
    "    if 'x0' not in X.columns:\n",
    "        x0 = np.ones((X.shape[0], 1), dtype=int)\n",
    "        X.insert(0, \"x0\", x0, True)\n",
    "    return(sigmoid(np.dot(X,theta)))\n",
    "\n",
    "\n",
    "# predict the y values using theta and X\n",
    "def predict(y, threshold):\n",
    "    y_pred_gd=[]\n",
    "    for i in y:\n",
    "        if i>=threshold:\n",
    "            y_pred_gd.append(1)\n",
    "        else:\n",
    "            y_pred_gd.append(0)\n",
    "    return y_pred_gd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta = gradient_descent_log_reg(X_train, y_train, 0.3, 10)\n",
    "# print(cross_entropy_obj(X_train,y_train,theta)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy for alpha = 0.01 and iterations =  10 is:  0.6495252510572939\n",
      "Cross Entropy for alpha = 0.01 and iterations =  50 is:  0.5365732045629628\n",
      "Cross Entropy for alpha = 0.01 and iterations =  100 is:  0.46158848382339607\n",
      "Accuracy when decision threshold is 0.01 and iter = 100:  0.8992180712423979\n",
      "F1_score when decision threshold is 0.01 and iter = 100:  0.8719646799116998\n",
      "\n",
      "\n",
      "Cross Entropy for alpha = 0.05 and iterations =  10 is:  0.5348795325629325\n",
      "Cross Entropy for alpha = 0.05 and iterations =  50 is:  0.36599881153810176\n",
      "Cross Entropy for alpha = 0.05 and iterations =  100 is:  0.3126833535736034\n",
      "Accuracy when decision threshold is 0.05 and iter = 100:  0.9026933101650738\n",
      "F1_score when decision threshold is 0.05 and iter = 100:  0.8747203579418344\n",
      "\n",
      "\n",
      "Cross Entropy for alpha = 0.4 and iterations =  10 is:  0.3232409114693962\n",
      "Cross Entropy for alpha = 0.4 and iterations =  50 is:  0.2510707094697467\n",
      "Cross Entropy for alpha = 0.4 and iterations =  100 is:  0.23462056796440348\n",
      "Accuracy when decision threshold is 0.4 and iter = 100:  0.9131190269331017\n",
      "F1_score when decision threshold is 0.4 and iter = 100:  0.8876404494382022\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_rates = [0.01, 0.05, 0.4]\n",
    "iter_var = [10, 50, 100]\n",
    "\n",
    "for i in lr_rates:\n",
    "    for j in iter_var:\n",
    "        theta = gradient_descent_log_reg(X_train, y_train, i, j)\n",
    "        print(\"Cross Entropy for alpha =\", i,\"and iterations = \", j, \"is: \", cross_entropy_obj(X_train,y_train,theta)[0][0])\n",
    "        if j == 100:\n",
    "            pred_prob = predict_prob(X_test,theta)\n",
    "            y_pred_val = predict(pred_prob, 0.5)\n",
    "            print(\"Accuracy when decision threshold is\",i,\"and iter = 100: \",metrics.accuracy_score(y_test, y_pred_val))\n",
    "            print(\"F1_score when decision threshold is\",i,\"and iter = 100: \",metrics.f1_score(y_test, y_pred_val))\n",
    "            print(\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the metrics given by the package, the accuracy and F1 score obtained is less using my implementation of the logistic regression using gradient descent which might be due to the fact that the gradient descent is still not converged with 100 iterations and the alpha values selected.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing bias again\n",
    "\n",
    "# X_train = X_train.iloc[:,1:]\n",
    "# X_test = X_test.iloc[:,1:]\n",
    "if X_train.columns[0] == \"x0\":\n",
    "    X_train = X_train.iloc[:,1:]\n",
    "if X_test.columns[0]==\"x0\":\n",
    "    X_test = X_test.iloc[:,1:]\n",
    "y_train = np.ravel(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in training data using KNN for k= 1 :  0.9994202898550725\n",
      "Error in training data using KNN for k= 1 :  0.0005797101449275255\n",
      "Accuracy in testing data using KNN for k= 1 :  0.9183318853171155\n",
      "Error in testing data using KNN for k= 1 :  0.08166811468288449\n",
      "\n",
      "\n",
      "Accuracy in training data using KNN for k= 3 :  0.9504347826086956\n",
      "Error in training data using KNN for k= 3 :  0.04956521739130437\n",
      "Accuracy in testing data using KNN for k= 3 :  0.9252823631624674\n",
      "Error in testing data using KNN for k= 3 :  0.07471763683753263\n",
      "\n",
      "\n",
      "Accuracy in training data using KNN for k= 5 :  0.9307246376811594\n",
      "Error in training data using KNN for k= 5 :  0.06927536231884057\n",
      "Accuracy in testing data using KNN for k= 5 :  0.9235447437011295\n",
      "Error in testing data using KNN for k= 5 :  0.07645525629887051\n",
      "\n",
      "\n",
      "Accuracy in training data using KNN for k= 7 :  0.9260869565217391\n",
      "Error in training data using KNN for k= 7 :  0.07391304347826089\n",
      "Accuracy in testing data using KNN for k= 7 :  0.9087749782797567\n",
      "Error in testing data using KNN for k= 7 :  0.09122502172024327\n",
      "\n",
      "\n",
      "Accuracy in training data using KNN for k= 9 :  0.9234782608695652\n",
      "Error in training data using KNN for k= 9 :  0.07652173913043481\n",
      "Accuracy in testing data using KNN for k= 9 :  0.9096437880104257\n",
      "Error in testing data using KNN for k= 9 :  0.09035621198957433\n",
      "\n",
      "\n",
      "Accuracy in training data using KNN for k= 11 :  0.9214492753623188\n",
      "Error in training data using KNN for k= 11 :  0.0785507246376812\n",
      "Accuracy in testing data using KNN for k= 11 :  0.9087749782797567\n",
      "Error in testing data using KNN for k= 11 :  0.09122502172024327\n",
      "\n",
      "\n",
      "Accuracy in training data using KNN for k= 13 :  0.9162318840579711\n",
      "Error in training data using KNN for k= 13 :  0.08376811594202893\n",
      "Accuracy in testing data using KNN for k= 13 :  0.9131190269331017\n",
      "Error in testing data using KNN for k= 13 :  0.08688097306689835\n",
      "\n",
      "\n",
      "Accuracy in training data using KNN for k= 15 :  0.9098550724637681\n",
      "Error in training data using KNN for k= 15 :  0.09014492753623193\n",
      "Accuracy in testing data using KNN for k= 15 :  0.9009556907037359\n",
      "Error in testing data using KNN for k= 15 :  0.09904430929626407\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "for i in range(1,17,2):\n",
    "    classifier = KNeighborsClassifier(n_neighbors=i)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    #predictions \n",
    "    y_pred_train_knn = classifier.predict(X_train)\n",
    "    y_pred_test_knn = classifier.predict(X_test)\n",
    "    print(\"Accuracy in training data using KNN for k=\",i,\": \",metrics.accuracy_score(y_train, y_pred_train_knn))\n",
    "    print(\"Error in training data using KNN for k=\",i,\": \",1-metrics.accuracy_score(y_train, y_pred_train_knn))\n",
    "    print(\"Accuracy in testing data using KNN for k=\",i,\": \",metrics.accuracy_score(y_test, y_pred_test_knn))\n",
    "    print(\"Error in testing data using KNN for k=\",i,\": \",1-metrics.accuracy_score(y_test, y_pred_test_knn))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k=3, gives the highest accuracy in testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in training set using Logistic Regression : 0.9318840579710145\n",
      "Error in training set using Logistic Regression : 0.06811594202898552\n",
      "Accuracy in testing set using Logistic Regression : 0.9374456993918332\n",
      "Error in testing set using Logistic Regression : 0.0625543006081668\n",
      "Precision in testing set using Logistic Regression : 0.9321266968325792\n",
      "Recall in testing set using Logistic Regression : 0.9074889867841409\n",
      "\n",
      "\n",
      "Accuracy in training set using LDA : 0.896231884057971\n",
      "Error in training set using LDA : 0.10376811594202895\n",
      "Accuracy in testing set using LDA : 0.894005212858384\n",
      "Error in testing set using LDA : 0.10599478714161603\n",
      "Precision in testing set using LDA : 0.9088669950738916\n",
      "Recall in testing set using LDA : 0.8127753303964758\n",
      "\n",
      "\n",
      "Accuracy in training set using kNN : 0.9504347826086956\n",
      "Error in training set using kNN : 0.04956521739130437\n",
      "Accuracy in testing set using kNN : 0.9252823631624674\n",
      "Error in testing set using kNN : 0.07471763683753263\n",
      "Precision in testing set using kNN : 0.9088888888888889\n",
      "Recall in testing set using kNN : 0.9008810572687225\n",
      "\n",
      "\n",
      "Accuracy in training set using Naive Bayes : 0.8136231884057971\n",
      "Error in training set using Naive Bayes : 0.18637681159420294\n",
      "Accuracy in testing set using Naive Bayes : 0.8166811468288445\n",
      "Error in testing set using Naive Bayes : 0.18331885317115548\n",
      "Precision in testing set using Naive Bayes : 0.6925515055467512\n",
      "Recall in testing set using Naive Bayes : 0.9625550660792952\n",
      "\n",
      "\n",
      "Accuracy in training set using Decision Tree : 0.9994202898550725\n",
      "Error in training set using Decision Tree : 0.0005797101449275255\n",
      "Accuracy in testing set using Decision Tree : 0.9079061685490878\n",
      "Error in testing set using Decision Tree : 0.09209383145091221\n",
      "Precision in testing set using Decision Tree : 0.8670886075949367\n",
      "Recall in testing set using Decision Tree : 0.9052863436123348\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "classifiers = [\"Logistic Regression\", \"LDA\", \"kNN\", \"Naive Bayes\", \"Decision Tree\"]\n",
    "\n",
    "def log_reg(X,y):\n",
    "    log_reg = LogisticRegression()\n",
    "    log_reg.fit(X,y)\n",
    "    return log_reg\n",
    "\n",
    "def knn(X,y):\n",
    "    classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "    classifier.fit(X,y)\n",
    "    return classifier\n",
    "\n",
    "def LDA(X,y):\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    lda.fit(X,y)\n",
    "    return lda\n",
    "\n",
    "def NB(X,y):\n",
    "    nb = GaussianNB()\n",
    "    nb.fit(X,y)\n",
    "    return nb\n",
    "\n",
    "def DTClassifier(X,y):\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(X,y)\n",
    "    return clf\n",
    "\n",
    "\n",
    "    \n",
    "def predict(classifier,X_train,y_train,X_test,y_test):\n",
    "    if classifier == \"Logistic Regression\":\n",
    "        model = log_reg(X_train,y_train)\n",
    "    elif classifier == \"kNN\":\n",
    "        model = knn(X_train, y_train)\n",
    "    elif classifier == \"LDA\" :\n",
    "        model = LDA(X_train, y_train)\n",
    "    elif classifier == \"Naive Bayes\":\n",
    "        model = NB(X_train, y_train)\n",
    "    elif classifier == \"Decision Tree\":\n",
    "        model = DTClassifier(X_train, y_train)\n",
    "    \n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    print(\"Accuracy in training set using\",classifier,\":\",metrics.accuracy_score(y_train, y_pred_train))\n",
    "    print(\"Error in training set using\",classifier,\":\",1 - metrics.accuracy_score(y_train, y_pred_train))\n",
    "    print(\"Accuracy in testing set using\",classifier,\":\",metrics.accuracy_score(y_test, y_pred_test))\n",
    "    print(\"Error in testing set using\",classifier,\":\",1 - metrics.accuracy_score(y_test, y_pred_test))\n",
    "    print(\"Precision in testing set using\",classifier,\":\",metrics.precision_score(y_test, y_pred_test))\n",
    "    print(\"Recall in testing set using\",classifier,\":\",metrics.recall_score(y_test, y_pred_test))\n",
    "    print(\"\\n\") \n",
    "    \n",
    "for i in classifiers:\n",
    "    predict(i,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is performing best with accuracy = 0.937, precision = 0.93 and recall = 0.907. Naive Bayes is performing worst among the models selected accuracy = 0.816, precision = 0.69 and recall = 0.96. Naive Bayes predicts the actual class correctly far better than any other classifiers but the accuracy and precision is lower than any other classifier probably due to the fact that Naive Bayes assumption of independence among features. The accuracy for LDA is 0.89, accuracy for kNN = 0.925 and accuracy for decsion trees = 0.910."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for Logistic Regression : 0.9712913746136683\n",
      "AUC for LDA : 0.9456402202011135\n",
      "AUC for kNN : 0.9552408370676087\n",
      "AUC for Naive Bayes : 0.9378377438866381\n",
      "AUC for Decision Tree : 0.9117536452638431\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def ROC(classifiers,X_train,y_train,X_test,y_test):\n",
    "    for classifier in classifiers:\n",
    "        if classifier == \"Logistic Regression\":\n",
    "            model = log_reg(X_train,y_train)\n",
    "        elif classifier == \"kNN\":\n",
    "            model = knn(X_train, y_train)\n",
    "        elif classifier == \"LDA\" :\n",
    "            model = LDA(X_train, y_train)\n",
    "        elif classifier == \"Naive Bayes\":\n",
    "            model = NB(X_train, y_train)\n",
    "        elif classifier == \"Decision Tree\":\n",
    "            model = DTClassifier(X_train, y_train)\n",
    "\n",
    "        #y_pred_test = model.predict(X_test)\n",
    "        y_pred_test = model.predict_proba(X_test)[:,1]\n",
    "        #print(y_pred_test)\n",
    "        fpr, tpr, thresholds = roc_curve(y_test,y_pred_test)\n",
    "        #print(fpr,tpr,thresholds)\n",
    "    #     plot_ROC(fpr,tpr,classifier)\n",
    "\n",
    "    # def plot_ROC(false_positive_rate,true_positive_rate,classifier): (area = %0.2f) % auc(fpr,tpr)\n",
    "        lw=2\n",
    "        plt.plot(fpr,tpr, lw=lw,\n",
    "                 label='ROC curve for %s (area = %0.2f)' % (classifier,auc(fpr,tpr)) )\n",
    "        plt.plot([0, 1], [0, 1], lw=lw, color='navy', linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        title = 'Receiver operating characteristic'\n",
    "        plt.title(title)\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        #plt.show()\n",
    "        print(\"AUC for\",classifier,\":\",auc(fpr,tpr))\n",
    "    plt.show()    \n",
    "    \n",
    "#for i in classifiers:\n",
    "ROC(classifiers,X_train,y_train,X_test,y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = org_data.iloc[:,0:-1]\n",
    "y = org_data[['class']]\n",
    "# splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 100, train_size = 100, stratify=y, random_state = 57)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "ss_scaler = preprocessing.StandardScaler()\n",
    "X_train = pd.DataFrame(ss_scaler.fit_transform(X_train),columns=X_train.columns)\n",
    "X_test = pd.DataFrame(ss_scaler.transform(X_test),columns = X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a\n",
    "def euclidean_distance(X,Y):\n",
    "    temp = np.transpose(X-Y).dot(X-Y)\n",
    "    distance = np.sqrt(temp)\n",
    "    return distance\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#b\n",
    "\n",
    "def test_kNN(k,train_feature,train_class,test_point):\n",
    "    distance_list = []\n",
    "    nearest_points = []\n",
    "    pred_class_dict = {}\n",
    "    \n",
    "    for i, sample in enumerate(train_feature.values):\n",
    "        distance_list.append([euclidean_distance(sample,test_point.values)])\n",
    "        distance_list[i].append(i)\n",
    "        \n",
    "    k_nearest_distances = sorted(distance_list,key=key_func)[0:k]\n",
    "    \n",
    "    for i in k_nearest_distances:\n",
    "        nearest_points.append(i[1])\n",
    "    \n",
    "    for point in nearest_points:\n",
    "        class_val = train_class.iloc[point][0]\n",
    "        if class_val not in pred_class_dict:\n",
    "            pred_class_dict[class_val] = 1\n",
    "        else:\n",
    "            pred_class_dict[class_val] += 1\n",
    "    #print(pred_class_dict)\n",
    "    pred_class = max(pred_class_dict, key=pred_class_dict.get)\n",
    "    return(pred_class)      \n",
    "\n",
    "    \n",
    "def key_func(s):\n",
    "    return s[0]\n",
    "\n",
    "print(test_kNN(5, X_train, y_train, X_test.iloc[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time of kNN testing averaged over all the points in the testing set for k=1 is 0.0032459044456481932 seconds \n",
      "Accuracy in testing data using KNN for k= 1 :  0.77\n",
      "Error in testing data using KNN for k= 1 :  0.22999999999999998\n",
      "\n",
      "\n",
      "Running time of kNN testing averaged over all the points in the testing set for k=3 is 0.003396103382110596 seconds \n",
      "Accuracy in testing data using KNN for k= 3 :  0.88\n",
      "Error in testing data using KNN for k= 3 :  0.12\n",
      "\n",
      "\n",
      "Running time of kNN testing averaged over all the points in the testing set for k=5 is 0.005052683353424073 seconds \n",
      "Accuracy in testing data using KNN for k= 5 :  0.87\n",
      "Error in testing data using KNN for k= 5 :  0.13\n",
      "\n",
      "\n",
      "Running time of kNN testing averaged over all the points in the testing set for k=7 is 0.00550652027130127 seconds \n",
      "Accuracy in testing data using KNN for k= 7 :  0.89\n",
      "Error in testing data using KNN for k= 7 :  0.10999999999999999\n",
      "\n",
      "\n",
      "Running time of kNN testing averaged over all the points in the testing set for k=9 is 0.007336332798004151 seconds \n",
      "Accuracy in testing data using KNN for k= 9 :  0.86\n",
      "Error in testing data using KNN for k= 9 :  0.14\n",
      "\n",
      "\n",
      "Running time of kNN testing averaged over all the points in the testing set for k=11 is 0.008636469841003419 seconds \n",
      "Accuracy in testing data using KNN for k= 11 :  0.82\n",
      "Error in testing data using KNN for k= 11 :  0.18000000000000005\n",
      "\n",
      "\n",
      "Running time of kNN testing averaged over all the points in the testing set for k=13 is 0.009068861007690429 seconds \n",
      "Accuracy in testing data using KNN for k= 13 :  0.86\n",
      "Error in testing data using KNN for k= 13 :  0.14\n",
      "\n",
      "\n",
      "Running time of kNN testing averaged over all the points in the testing set for k=15 is 0.010600345134735107 seconds \n",
      "Accuracy in testing data using KNN for k= 15 :  0.9\n",
      "Error in testing data using KNN for k= 15 :  0.09999999999999998\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# c\n",
    "# Using my own implementation\n",
    "import time\n",
    "\n",
    "n_row = X_test.shape[0]\n",
    "for k in range(1,17,2):\n",
    "    y_pred_kNN = []\n",
    "    start_time = time.time()\n",
    "    for row in range(n_row):\n",
    "        y_pred_kNN.append(test_kNN(k, X_train, y_train, X_test.iloc[row]))\n",
    "    print(\"Running time of kNN testing averaged over all the points in the testing set for k=%d is %s seconds \" \n",
    "          % (k,((time.time() - start_time)/n_row)))\n",
    "    #print(y_pred_kNN)    \n",
    "    print(\"Accuracy in testing data using KNN for k=\",k,\": \",metrics.accuracy_score(y_test,y_pred_kNN))\n",
    "    print(\"Error in testing data using KNN for k=\",k,\": \",1-metrics.accuracy_score(y_test, y_pred_kNN))\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in testing data using KNN for k= 1 :  0.77\n",
      "Error in testing data using KNN for k= 1 :  0.22999999999999998\n",
      "\n",
      "\n",
      "Accuracy in testing data using KNN for k= 3 :  0.88\n",
      "Error in testing data using KNN for k= 3 :  0.12\n",
      "\n",
      "\n",
      "Accuracy in testing data using KNN for k= 5 :  0.87\n",
      "Error in testing data using KNN for k= 5 :  0.13\n",
      "\n",
      "\n",
      "Accuracy in testing data using KNN for k= 7 :  0.89\n",
      "Error in testing data using KNN for k= 7 :  0.10999999999999999\n",
      "\n",
      "\n",
      "Accuracy in testing data using KNN for k= 9 :  0.86\n",
      "Error in testing data using KNN for k= 9 :  0.14\n",
      "\n",
      "\n",
      "Accuracy in testing data using KNN for k= 11 :  0.82\n",
      "Error in testing data using KNN for k= 11 :  0.18000000000000005\n",
      "\n",
      "\n",
      "Accuracy in testing data using KNN for k= 13 :  0.86\n",
      "Error in testing data using KNN for k= 13 :  0.14\n",
      "\n",
      "\n",
      "Accuracy in testing data using KNN for k= 15 :  0.9\n",
      "Error in testing data using KNN for k= 15 :  0.09999999999999998\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# d\n",
    "#Using existing package\n",
    "y_train = np.ravel(y_train)\n",
    "for k in range(1,17,2):\n",
    "    classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    #predictions \n",
    "    #y_pred_train_knn = classifier.predict(X_train)\n",
    "    y_pred_test_knn = classifier.predict(X_test)\n",
    "    #print(\"Accuracy in training data using KNN for k=\",i,\": \",metrics.accuracy_score(y_train, y_pred_train_knn))\n",
    "    #print(\"Error in training data using KNN for k=\",i,\": \",1-metrics.accuracy_score(y_train, y_pred_train_knn))\n",
    "    print(\"Accuracy in testing data using KNN for k=\",k,\": \",metrics.accuracy_score(y_test, y_pred_test_knn))\n",
    "    print(\"Error in testing data using KNN for k=\",k,\": \",1-metrics.accuracy_score(y_test, y_pred_test_knn))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result obtained using my own implementation of kNN is similar to those obtained using the package for different values of k. The accuracy increase till k=7, post which the accuracy decreases on increasing k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The running time of kNN testing averaged over all the points in the testing set is printed along with problem 3c.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = org_data.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation error using Logistic Regression for number of folds = 5 : 0.1408695652173913\n",
      "Average validation error using Logistic Regression for number of folds = 10 : 0.10934782608695652\n",
      "Average validation error using LDA for number of folds = 5 : 0.18391304347826087\n",
      "Average validation error using LDA for number of folds = 10 : 0.14804347826086955\n"
     ]
    }
   ],
   "source": [
    "def k_fold_CV(data,k_fold,classifier):\n",
    "    N = data.shape[0]\n",
    "    fold_size = int(N/k_fold)\n",
    "    start = 0\n",
    "    end = fold_size\n",
    "    error = 0\n",
    "    for k in range(k_fold):\n",
    "#         print(\"start\",start)\n",
    "#         print(\"end\",end)\n",
    "        test = data[start:end]\n",
    "        if start != 0:\n",
    "            train = data[0:start]\n",
    "            train = train.append(data[end:N],ignore_index = True)\n",
    "        else:    \n",
    "            train = data[end:N]\n",
    "#         print(\"test shape\",test.shape)\n",
    "#         print(\"train shape\",train.shape)\n",
    "#         print(\"\\n\")\n",
    "        X_train, X_test, y_train, y_test = preprocessing_data(train,test)\n",
    "#         print(\"y_train shape\",y_train.shape)\n",
    "#         print(\"y_test shape\", y_test.shape)\n",
    "        if classifier == \"LDA\":\n",
    "            model = LDA(X_train,y_train)\n",
    "        elif classifier == \"Logistic Regression\":\n",
    "            model = Log_Reg(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "#         print(\"Error in testing set using\",model,\"for fold\",k,\":\",1 - metrics.accuracy_score(y_test, y_pred))\n",
    "        error += 1 - metrics.accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        start = start + fold_size\n",
    "        if end + fold_size > N:\n",
    "            end = N\n",
    "        else:\n",
    "            end = end + fold_size\n",
    "    print(\"Average validation error using\",classifier,\"for number of folds =\",k_fold,\":\", error/k_fold)    \n",
    "    return (error/k_fold)\n",
    "   \n",
    "    \n",
    "def LDA(X,y):\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    lda.fit(X,y)\n",
    "    return lda\n",
    "\n",
    "def Log_Reg(X,y):\n",
    "    log_reg = LogisticRegression()\n",
    "    log_reg.fit(X,y)\n",
    "    return log_reg\n",
    "\n",
    "def preprocessing_data(train,test):\n",
    "    X_train = train.iloc[:,0:-1]\n",
    "    y_train = train[\"class\"]\n",
    "    X_test = test.iloc[:,0:-1]\n",
    "    y_test = test[\"class\"]\n",
    "    ss_scaler = preprocessing.StandardScaler()\n",
    "    X_train = pd.DataFrame(ss_scaler.fit_transform(X_train),columns=X_train.columns)\n",
    "    X_test = pd.DataFrame(ss_scaler.transform(X_test),columns = X_test.columns) \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "    \n",
    "#k_fold_CV(data,5,\"LDA\") \n",
    "classifiers = [\"Logistic Regression\", \"LDA\"]\n",
    "for model in classifiers:\n",
    "    for k in [5,10]:\n",
    "        k_fold_CV(data,k,model)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above run, we can say that the Logistic Regression works better than LDA as the average validation error for k=5 using logistic regression is 0.140 and using LDA is 0.183. And the trend is similar for k = 10, however the average validation error decreases as the value of k increases from 5 to 10 for both the classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recieve A in Class with probability: 0.3775406687981454\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "theta = np.matrix('-6;0.05;1')\n",
    "\n",
    "X = np.matrix('1;40;3.5')\n",
    "\n",
    "temp = np.transpose(theta).dot(X)\n",
    "\n",
    "prob = sigmoid(temp)\n",
    "\n",
    "print(\"Recieve A in Class with probability:\", prob.A1[0])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
